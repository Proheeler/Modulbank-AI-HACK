{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from sklearn import ensemble, cross_validation, learning_curve, metrics \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# data review\n",
    "\n",
    "train_df = pd.read_csv('train.csv',sep='\\t')\n",
    "test_df = pd.read_csv('test.csv',sep='\\t')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=train_df.drop(['0'], axis=1)\n",
    "Y=train_df['0']\n",
    "X=X.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "test_df = pd.read_csv('test.csv',sep='\\t')\n",
    "X_val=test_df.drop(['Unnamed: 0'], axis=1)\n",
    "X_val=X_val.drop(['0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop=['83',\n",
    " '54',\n",
    " '345',\n",
    " '75',\n",
    " '49',\n",
    " '52',\n",
    " '86',\n",
    " '67',\n",
    " '203',\n",
    " '190',\n",
    " '74',\n",
    " '245',\n",
    " '295',\n",
    " '271',\n",
    " '108',\n",
    " '136',\n",
    " '292',\n",
    " '337',\n",
    " '231',\n",
    " '88',\n",
    " '225',\n",
    " '40',\n",
    " '184',\n",
    " '32',\n",
    " '66',\n",
    " '72',\n",
    " '95',\n",
    " '36',\n",
    " '29',\n",
    " '81',\n",
    " '168',\n",
    " '142',\n",
    " '34',\n",
    " '266',\n",
    " '286',\n",
    " '159',\n",
    " '98',\n",
    " '262',\n",
    " '37',\n",
    " '331',\n",
    " '91',\n",
    " '189',\n",
    " '14',\n",
    " '80',\n",
    " '78',\n",
    " '176',\n",
    " '33',\n",
    " '112',\n",
    " '208',\n",
    " '101',\n",
    " '96',\n",
    " '56',\n",
    " '82',\n",
    " '84',\n",
    " '306',\n",
    " '272',\n",
    " '318',\n",
    " '280',\n",
    " '53',\n",
    " '158',\n",
    " '13',\n",
    " '315',\n",
    " '243',\n",
    " '281',\n",
    " '330',\n",
    " '237',\n",
    " '153',\n",
    " '17',\n",
    " '329',\n",
    " '63',\n",
    " '89',\n",
    " '239',\n",
    " '147',\n",
    " '143',\n",
    " '307',\n",
    " '2',\n",
    " '9',\n",
    " '133',\n",
    " '333',\n",
    " '129',\n",
    " '288',\n",
    " '264',\n",
    " '1',\n",
    " '8',\n",
    " '229',\n",
    " '99',\n",
    " '7',\n",
    " '282',\n",
    " '6',\n",
    " '68',\n",
    " '122',\n",
    " '283',\n",
    " '248',\n",
    " '311',\n",
    " '154',\n",
    " '199',\n",
    " '46',\n",
    " '299',\n",
    " '21',\n",
    " '335',\n",
    " '222',\n",
    " '226',\n",
    " '22',\n",
    " '198',\n",
    " '134',\n",
    " '58',\n",
    " '267',\n",
    " '155',\n",
    " '45',\n",
    " '322',\n",
    " '209',\n",
    " '55',\n",
    " '207',\n",
    " '314',\n",
    " '252',\n",
    " '71',\n",
    " '291',\n",
    " '200',\n",
    " '210',\n",
    " '166',\n",
    " '313',\n",
    " '261',\n",
    " '177',\n",
    " '293',\n",
    " '109',\n",
    " '310',\n",
    " '343',\n",
    " '93',\n",
    " '230',\n",
    " '85',\n",
    " '59',\n",
    " '87',\n",
    " '206',\n",
    " '202',\n",
    " '23',\n",
    " '92',\n",
    " '57',\n",
    " '325',\n",
    " '247',\n",
    " '179',\n",
    " '114',\n",
    " '241',\n",
    " '340',\n",
    " '157',\n",
    " '279',\n",
    " '149',\n",
    " '170',\n",
    " '135',\n",
    " '186',\n",
    " '160']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val=X_val.drop(features_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.drop(features_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.22222222, 0.11111111, 0.44444444, 0.        , 0.66666667,\n",
       "       0.33333333, 0.55555556, 1.        , 0.77777778, 0.88888889])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['341'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "395\n",
      "803\n",
      "1462\n",
      "680\n",
      "1020\n",
      "236\n",
      "1465\n",
      "191\n",
      "2\n",
      "307\n",
      "991\n",
      "346\n",
      "1152\n",
      "2509\n",
      "21\n",
      "23\n",
      "1125\n",
      "19376\n",
      "2175\n",
      "11562\n",
      "462\n",
      "5\n",
      "361\n",
      "274\n",
      "11\n",
      "355\n",
      "462\n",
      "5\n",
      "454\n",
      "326\n",
      "10\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "496\n",
      "315\n",
      "19494\n",
      "19475\n",
      "19501\n",
      "19502\n",
      "19490\n",
      "112\n",
      "5\n",
      "78\n",
      "80\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "12\n",
      "2\n",
      "85\n",
      "81\n",
      "62\n",
      "85\n",
      "82\n",
      "85\n",
      "85\n",
      "85\n",
      "84\n",
      "2\n",
      "85\n",
      "83\n",
      "2\n",
      "2\n",
      "10\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in X.columns:\n",
    "    print len(X[i].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>73</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>79</th>\n",
       "      <th>90</th>\n",
       "      <th>94</th>\n",
       "      <th>100</th>\n",
       "      <th>102</th>\n",
       "      <th>...</th>\n",
       "      <th>342_1_1_0</th>\n",
       "      <th>342_1_1_1</th>\n",
       "      <th>344_0_0_0</th>\n",
       "      <th>344_0_0_1</th>\n",
       "      <th>344_0_1_0</th>\n",
       "      <th>344_0_1_1</th>\n",
       "      <th>344_1_0_0</th>\n",
       "      <th>344_1_0_1</th>\n",
       "      <th>344_1_1_0</th>\n",
       "      <th>344_1_1_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.410139</td>\n",
       "      <td>0.352568</td>\n",
       "      <td>0.011460</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15430</td>\n",
       "      <td>0.1543</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.440312</td>\n",
       "      <td>0.352568</td>\n",
       "      <td>0.036168</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.36515</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.18257</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.54772</td>\n",
       "      <td>0.372660</td>\n",
       "      <td>0.146644</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.09206</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.09206</td>\n",
       "      <td>0.512210</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.032805</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.473152</td>\n",
       "      <td>0.352568</td>\n",
       "      <td>0.110224</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1560 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    69   70       73      76       77   79       90        94       100  \\\n",
       "0  0.0  0.0  0.00000  0.0000  0.00000  0.0  0.00000  0.410139  0.352568   \n",
       "1  0.0  0.0  0.15430  0.1543  0.00000  0.0  0.00000  0.440312  0.352568   \n",
       "2  0.0  0.0  0.36515  0.0000  0.18257  0.0  0.54772  0.372660  0.146644   \n",
       "3  0.0  0.0  0.00000  0.0000  0.09206  0.0  0.09206  0.512210  1.000000   \n",
       "4  0.0  0.0  0.00000  0.0000  0.00000  0.0  0.00000  0.473152  0.352568   \n",
       "\n",
       "        102    ...      342_1_1_0  342_1_1_1  344_0_0_0  344_0_0_1  344_0_1_0  \\\n",
       "0  0.011460    ...              0          1          0          1          1   \n",
       "1  0.036168    ...              0          1          0          1          1   \n",
       "2  0.000024    ...              0          1          0          1          1   \n",
       "3  0.032805    ...              0          1          0          1          1   \n",
       "4  0.110224    ...              0          1          0          1          1   \n",
       "\n",
       "   344_0_1_1  344_1_0_0  344_1_0_1  344_1_1_0  344_1_1_1  \n",
       "0          0          1          0          0          1  \n",
       "1          0          1          0          0          1  \n",
       "2          0          1          0          0          1  \n",
       "3          0          1          0          0          1  \n",
       "4          0          1          0          0          1  \n",
       "\n",
       "[5 rows x 1560 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_columns = []\n",
    "for i in X.columns: \n",
    "    if(len(X[i].unique())<=30): \n",
    "        cat_columns.append(i)\n",
    "\n",
    "X = pd.get_dummies(data=X, columns=cat_columns)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X, Y, train_size=0.7, random_state=1999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "clf = MLPRegressor(solver='lbfgs', alpha=1e-2,hidden_layer_sizes=(10, 1), random_state=1,activation='logistic',learning_rate='invscaling',learning_rate_init=0.00001,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7416974747833835"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X, Y)  \n",
    "roc_auc_score(Y, clf.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>73</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>79</th>\n",
       "      <th>90</th>\n",
       "      <th>94</th>\n",
       "      <th>100</th>\n",
       "      <th>102</th>\n",
       "      <th>...</th>\n",
       "      <th>341_0.444444444</th>\n",
       "      <th>341_0.555555556</th>\n",
       "      <th>341_0.666666667</th>\n",
       "      <th>341_0.777777778</th>\n",
       "      <th>341_0.888888889</th>\n",
       "      <th>341_1.0</th>\n",
       "      <th>342_0</th>\n",
       "      <th>342_1</th>\n",
       "      <th>344_0</th>\n",
       "      <th>344_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.35921</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.17961</td>\n",
       "      <td>0.457598</td>\n",
       "      <td>0.389702</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.357848</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.160103</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.215445</td>\n",
       "      <td>0.003507</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.352568</td>\n",
       "      <td>0.035777</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.170736</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.649930</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 398 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    69       70   73   76   77   79       90        94       100       102  \\\n",
       "0  0.0  0.35921  0.0  0.0  0.0  0.0  0.17961  0.457598  0.389702  0.046700   \n",
       "1  0.0  0.00000  0.0  0.0  0.0  0.0  0.00000  0.357848  1.000000  0.160103   \n",
       "2  0.0  0.00000  0.0  0.0  0.0  0.0  0.00000  0.215445  0.003507  0.000226   \n",
       "3  0.0  0.00000  0.0  0.0  0.0  0.0  0.00000  0.000000  0.352568  0.035777   \n",
       "4  0.0  0.00000  0.0  0.0  0.0  0.0  0.00000  0.170736  1.000000  0.649930   \n",
       "\n",
       "   ...    341_0.444444444  341_0.555555556  341_0.666666667  341_0.777777778  \\\n",
       "0  ...                  0                0                0                0   \n",
       "1  ...                  0                0                0                0   \n",
       "2  ...                  0                0                0                0   \n",
       "3  ...                  0                0                0                0   \n",
       "4  ...                  0                0                0                0   \n",
       "\n",
       "   341_0.888888889  341_1.0  342_0  342_1  344_0  344_1  \n",
       "0                0        0      0      1      0      1  \n",
       "1                0        0      0      1      0      1  \n",
       "2                0        0      0      1      0      1  \n",
       "3                0        0      0      1      0      1  \n",
       "4                0        0      0      1      0      1  \n",
       "\n",
       "[5 rows x 398 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_columns1 = []\n",
    "for i in X_val.columns: \n",
    "    if(len(X_val[i].unique())<=30): \n",
    "        cat_columns1.append(i)\n",
    "\n",
    "X_val = pd.get_dummies(data=X_val, columns=cat_columns1)\n",
    "X_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "labels ['3_0_0' '3_0_1' '3_1_0' '3_1_1' '4_0_0' '4_0_1' '4_1_0' '4_1_1' '5_0_0'\n '5_0_1' '5_1_0' '5_1_1' '10_0_0' '10_0_1' '10_1_0' '10_1_1' '11_0_0'\n '11_0_1' '11_1_0' '11_1_1' '12_0_0' '12_0_1' '12_1_0' '12_1_1' '15_0_0'\n '15_0_1' '15_1_0' '15_1_1' '16_0_0' '16_0_1' '16_1_0' '16_1_1' '18_0_0'\n '18_0_1' '18_1_0' '18_1_1' '19_0_0' '19_0_1' '19_1_0' '19_1_1' '20_0_0'\n '20_0_1' '20_1_0' '20_1_1' '24_0_0' '24_0_1' '24_1_0' '24_1_1' '25_0_0'\n '25_0_1' '25_1_0' '25_1_1' '26_0_0' '26_0_1' '26_1_0' '26_1_1' '27_0_0'\n '27_0_1' '27_1_0' '27_1_1' '28_0_0' '28_0_1' '28_1_0' '28_1_1' '30_0_0'\n '30_0_1' '30_1_0' '30_1_1' '31_0_0' '31_0_1' '31_1_0' '31_1_1' '35_0_0'\n '35_0_1' '35_1_0' '35_1_1' '38_0_0' '38_0_1' '38_1_0' '38_1_1' '39_0_0'\n '39_0_1' '39_1_0' '39_1_1' '41_0_0' '41_0_1' '41_1_0' '41_1_1' '42_0_0'\n '42_0_1' '42_1_0' '42_1_1' '43_0_0' '43_0_1' '43_1_0' '43_1_1' '44_0_0'\n '44_0_1' '44_1_0' '44_1_1' '47_0_0' '47_0_1' '47_1_0' '47_1_1' '48_0_0'\n '48_0_1' '48_1_0' '48_1_1' '50_0_0' '50_0_1' '50_1_0' '50_1_1' '51_0_0'\n '51_0_1' '51_1_0' '51_1_1' '60_0_0' '60_0_1' '60_1_0' '60_1_1' '61_0_0'\n '61_0_1' '61_1_0' '61_1_1' '62_0_0' '62_0_1' '62_1_0' '62_1_1' '64_0_0'\n '64_0_1' '64_1_0' '64_1_1' '65_0_0' '65_0_1' '65_1_0' '65_1_1' '97_0_0'\n '97_0_1' '97_1_0' '97_1_1' '106_0.0_0' '106_0.0_1' '106_0.05_0'\n '106_0.05_1' '106_0.1_0' '106_0.1_1' '106_0.15_0' '106_0.15_1'\n '106_0.2_0' '106_0.2_1' '106_0.25_0' '106_0.25_1' '106_0.3_0' '106_0.3_1'\n '106_0.35_0' '106_0.35_1' '106_0.4_0' '106_0.4_1' '106_0.45_0'\n '106_0.45_1' '106_0.5_0' '106_0.5_1' '106_0.55_0' '106_0.55_1'\n '106_0.6_0' '106_0.6_1' '106_0.65_0' '106_0.65_1' '106_0.7_0' '106_0.7_1'\n '106_0.75_0' '106_0.75_1' '106_0.8_0' '106_0.8_1' '106_0.85_0'\n '106_0.85_1' '106_0.9_0' '106_0.9_1' '106_0.95_0' '106_0.95_1'\n '106_1.0_0' '106_1.0_1' '107_0.0_0' '107_0.0_1' '107_0.043478261_0'\n '107_0.043478261_1' '107_0.086956522_0' '107_0.086956522_1'\n '107_0.130434783_0' '107_0.130434783_1' '107_0.173913043_0'\n '107_0.173913043_1' '107_0.217391304_0' '107_0.217391304_1'\n '107_0.260869565_0' '107_0.260869565_1' '107_0.304347826_0'\n '107_0.304347826_1' '107_0.347826087_0' '107_0.347826087_1'\n '107_0.391304348_0' '107_0.391304348_1' '107_0.434782609_0'\n '107_0.434782609_1' '107_0.47826087_0' '107_0.47826087_1'\n '107_0.52173913_0' '107_0.52173913_1' '107_0.565217391_0'\n '107_0.565217391_1' '107_0.608695652_0' '107_0.608695652_1'\n '107_0.652173913_0' '107_0.652173913_1' '107_0.695652174_0'\n '107_0.695652174_1' '107_0.739130435_0' '107_0.739130435_1'\n '107_0.782608696_0' '107_0.782608696_1' '107_0.826086957_0'\n '107_0.826086957_1' '107_0.869565217_0' '107_0.869565217_1'\n '107_0.956521739_0' '107_0.956521739_1' '107_1.0_0' '107_1.0_1'\n '117_0.0_0' '117_0.0_1' '117_0.25_0' '117_0.25_1' '117_0.5_0' '117_0.5_1'\n '117_0.75_0' '117_0.75_1' '117_1.0_0' '117_1.0_1' '120_0.0_0' '120_0.0_1'\n '120_0.010752688_0' '120_0.010752688_1' '120_0.021505376_0'\n '120_0.021505376_1' '120_0.032258065_0' '120_0.032258065_1'\n '120_0.043010753_0' '120_0.043010753_1' '120_0.053763441_0'\n '120_0.053763441_1' '120_0.064516129_0' '120_0.064516129_1'\n '120_0.075268817_0' '120_0.075268817_1' '120_0.086021505_0'\n '120_0.086021505_1' '120_0.096774194_0' '120_0.096774194_1'\n '120_0.107526882_0' '120_0.107526882_1' '124_0.0_0' '124_0.0_1'\n '124_0.25_0' '124_0.25_1' '124_0.5_0' '124_0.5_1' '124_0.75_0'\n '124_0.75_1' '124_1.0_0' '124_1.0_1' '127_0.0_0' '127_0.0_1'\n '127_0.010752688_0' '127_0.010752688_1' '127_0.021505376_0'\n '127_0.021505376_1' '127_0.032258065_0' '127_0.032258065_1'\n '127_0.043010753_0' '127_0.043010753_1' '127_0.053763441_0'\n '127_0.053763441_1' '127_0.064516129_0' '127_0.064516129_1'\n '127_0.075268817_0' '127_0.075268817_1' '127_0.086021505_0'\n '127_0.086021505_1' '127_0.096774194_0' '127_0.096774194_1' '128_0_0'\n '128_0_1' '128_1_0' '128_1_1' '130_0_0' '130_0_1' '130_1_0' '130_1_1'\n '131_0_0' '131_0_1' '131_1_0' '131_1_1' '132_0_0' '132_0_1' '132_1_0'\n '132_1_1' '137_0_0' '137_0_1' '137_1_0' '137_1_1' '138_0_0' '138_0_1'\n '138_1_0' '138_1_1' '139_0_0' '139_0_1' '139_1_0' '139_1_1' '140_0_1'\n '141_0_0' '141_0_1' '141_1_0' '141_1_1' '144_0_0' '144_0_1' '144_1_0'\n '144_1_1' '145_0_0' '145_0_1' '145_1_0' '145_1_1' '146_0_0' '146_0_1'\n '146_1_0' '146_1_1' '148_0_0' '148_0_1' '148_1_0' '148_1_1' '150_0_0'\n '150_0_1' '150_1_0' '150_1_1' '151_0_0' '151_0_1' '151_1_0' '151_1_1'\n '152_0_1' '156_0_0' '156_0_1' '156_1_0' '156_1_1' '161_0_0' '161_0_1'\n '161_1_0' '161_1_1' '162_0_0' '162_0_1' '162_1_0' '162_1_1' '163_0_0'\n '163_0_1' '163_1_0' '163_1_1' '164_0_1' '165_0_0' '165_0_1' '165_1_0'\n '165_1_1' '167_0_0' '167_0_1' '167_1_0' '167_1_1' '169_0_0' '169_0_1'\n '169_1_0' '169_1_1' '171_0_0' '171_0_1' '171_1_0' '171_1_1' '172_0_0'\n '172_0_1' '172_1_0' '172_1_1' '173_0_0' '173_0_1' '173_1_0' '173_1_1'\n '174_0_0' '174_0_1' '174_1_0' '174_1_1' '175_0_0' '175_0_1' '175_1_0'\n '175_1_1' '178_0_0' '178_0_1' '178_1_0' '178_1_1' '180_0_0' '180_0_1'\n '180_1_0' '180_1_1' '181_0_0' '181_0_1' '181_1_0' '181_1_1' '182_0_0'\n '182_0_1' '182_1_0' '182_1_1' '183_0_0' '183_0_1' '183_1_0' '183_1_1'\n '185_0_0' '185_0_1' '185_1_0' '185_1_1' '187_0_0' '187_0_1' '187_1_0'\n '187_1_1' '188_0_0' '188_0_1' '188_1_0' '188_1_1' '191_0_0' '191_0_1'\n '191_1_0' '191_1_1' '192_0_0' '192_0_1' '192_1_0' '192_1_1' '193_0_0'\n '193_0_1' '193_1_0' '193_1_1' '212_0.0_0' '212_0.0_1' '212_0.003846547_0'\n '212_0.003846547_1' '212_0.0260542_0' '212_0.0260542_1'\n '212_0.233191725_0' '212_0.233191725_1' '212_1.0_0' '212_1.0_1' '215_0_0'\n '215_0_1' '215_1_0' '215_1_1' '216_0_0' '216_0_1' '216_1_0' '216_1_1'\n '217_0_0' '217_0_1' '217_1_0' '217_1_1' '218_0_0' '218_0_1' '218_1_0'\n '218_1_1' '219_0_0' '219_0_1' '219_1_0' '219_1_1' '220_0_0' '220_0_1'\n '220_1_0' '220_1_1' '221_0_0' '221_0_1' '221_1_0' '221_1_1' '223_0_0'\n '223_0_1' '223_1_0' '223_1_1' '224_0_0' '224_0_1' '224_1_0' '224_1_1'\n '227_0_0' '227_0_1' '227_1_0' '227_1_1' '228_0_0' '228_0_1' '228_1_0'\n '228_1_1' '232_0_0' '232_0_1' '232_1_0' '232_1_1' '233_0_0' '233_0_1'\n '233_1_0' '233_1_1' '234_0_0' '234_0_1' '234_1_0' '234_1_1' '235_0_0'\n '235_0_1' '235_1_0' '235_1_1' '236_0_0' '236_0_1' '236_1_0' '236_1_1'\n '238_0_0' '238_0_1' '238_1_0' '238_1_1' '240_0_0' '240_0_1' '240_1_0'\n '240_1_1' '242_0_0' '242_0_1' '242_1_0' '242_1_1' '244_0_0' '244_0_1'\n '244_1_0' '244_1_1' '246_0_0' '246_0_1' '246_1_0' '246_1_1' '249_0_0'\n '249_0_1' '249_1_0' '249_1_1' '250_0_0' '250_0_1' '250_1_0' '250_1_1'\n '251_0_0' '251_0_1' '251_1_0' '251_1_1' '253_0_0' '253_0_1' '253_1_0'\n '253_1_1' '254_0_0' '254_0_1' '254_1_0' '254_1_1' '255_0_0' '255_0_1'\n '255_1_0' '255_1_1' '256_0_0' '256_0_1' '256_1_0' '256_1_1' '257_0_0'\n '257_0_1' '257_1_0' '257_1_1' '258_0_0' '258_0_1' '258_1_0' '258_1_1'\n '259_0_0' '259_0_1' '259_1_0' '259_1_1' '260_0_0' '260_0_1' '260_1_0'\n '260_1_1' '263_0_0' '263_0_1' '263_1_0' '263_1_1' '265_0_0' '265_0_1'\n '265_1_0' '265_1_1' '268_0_0' '268_0_1' '268_1_0' '268_1_1' '269_0_0'\n '269_0_1' '269_1_0' '269_1_1' '270_0_0' '270_0_1' '270_1_0' '270_1_1'\n '273_0_0' '273_0_1' '273_1_0' '273_1_1' '274_0_0' '274_0_1' '274_1_0'\n '274_1_1' '275_0_0' '275_0_1' '275_1_0' '275_1_1' '276_0_0' '276_0_1'\n '276_1_0' '276_1_1' '277_0_0' '277_0_1' '277_1_0' '277_1_1' '278_0_0'\n '278_0_1' '278_1_0' '278_1_1' '284_0_0' '284_0_1' '284_1_0' '284_1_1'\n '285_0_0' '285_0_1' '285_1_0' '285_1_1' '287_0_0' '287_0_1' '287_1_0'\n '287_1_1' '289_0_0' '289_0_1' '289_1_0' '289_1_1' '290_0_0' '290_0_1'\n '290_1_0' '290_1_1' '294_0_0' '294_0_1' '294_1_0' '294_1_1' '296_0_0'\n '296_0_1' '296_1_0' '296_1_1' '297_0_0' '297_0_1' '297_1_0' '297_1_1'\n '298_0_0' '298_0_1' '298_1_0' '298_1_1' '300_0_0' '300_0_1' '300_1_0'\n '300_1_1' '301_0_0' '301_0_1' '301_1_0' '301_1_1' '302_0_0' '302_0_1'\n '302_1_0' '302_1_1' '303_0_0' '303_0_1' '303_1_0' '303_1_1' '304_0_0'\n '304_0_1' '304_1_0' '304_1_1' '305_0_0' '305_0_1' '305_1_0' '305_1_1'\n '308_0_0' '308_0_1' '308_1_0' '308_1_1' '309_0_0' '309_0_1' '309_1_0'\n '309_1_1' '312_0.0_0' '312_0.0_1' '312_0.39424_0' '312_0.39424_1'\n '312_0.4324508_0' '312_0.4324508_1' '312_0.47097_0' '312_0.47097_1'\n '312_0.5197096_0' '312_0.5197096_1' '312_0.5722851_0' '312_0.5722851_1'\n '312_0.588824_0' '312_0.588824_1' '312_0.6608119_0' '312_0.6608119_1'\n '312_0.7575137_0' '312_0.7575137_1' '312_0.8421118_0' '312_0.8421118_1'\n '312_0.9310675_0' '312_0.9310675_1' '312_1.0_0' '312_1.0_1' '316_0_0'\n '316_0_1' '316_1_0' '316_1_1' '332_0_0' '332_0_1' '332_1_0' '332_1_1'\n '338_0_0' '338_0_1' '338_1_0' '338_1_1' '339_0_0' '339_0_1' '339_1_0'\n '339_1_1' '341_0.0_0' '341_0.0_1' '341_0.111111111_0' '341_0.111111111_1'\n '341_0.222222222_0' '341_0.222222222_1' '341_0.333333333_0'\n '341_0.333333333_1' '341_0.444444444_0' '341_0.444444444_1'\n '341_0.555555556_0' '341_0.555555556_1' '341_0.666666667_0'\n '341_0.666666667_1' '341_0.777777778_0' '341_0.777777778_1'\n '341_0.888888889_0' '341_0.888888889_1' '341_1.0_0' '341_1.0_1' '342_0_0'\n '342_0_1' '342_1_0' '342_1_1' '344_0_0' '344_0_1' '344_1_0' '344_1_1'] not contained in axis",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-87951154b953>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_tt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcat_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\1\\Anaconda2\\lib\\site-packages\\pandas\\core\\reshape\\reshape.pyc\u001b[0m in \u001b[0;36mget_dummies\u001b[1;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first)\u001b[0m\n\u001b[0;32m   1202\u001b[0m             \u001b[0mwith_dummies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1203\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1204\u001b[1;33m             \u001b[0mwith_dummies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns_to_encode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1206\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpre\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns_to_encode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix_sep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\1\\Anaconda2\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   2528\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2529\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2530\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2532\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\1\\Anaconda2\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   2560\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2561\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2562\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2563\u001b[0m             \u001b[0mdropped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2564\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\1\\Anaconda2\\lib\\site-packages\\pandas\\core\\indexes\\base.pyc\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   3742\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'ignore'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3743\u001b[0m                 raise ValueError('labels %s not contained in axis' %\n\u001b[1;32m-> 3744\u001b[1;33m                                  labels[mask])\n\u001b[0m\u001b[0;32m   3745\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3746\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: labels ['3_0_0' '3_0_1' '3_1_0' '3_1_1' '4_0_0' '4_0_1' '4_1_0' '4_1_1' '5_0_0'\n '5_0_1' '5_1_0' '5_1_1' '10_0_0' '10_0_1' '10_1_0' '10_1_1' '11_0_0'\n '11_0_1' '11_1_0' '11_1_1' '12_0_0' '12_0_1' '12_1_0' '12_1_1' '15_0_0'\n '15_0_1' '15_1_0' '15_1_1' '16_0_0' '16_0_1' '16_1_0' '16_1_1' '18_0_0'\n '18_0_1' '18_1_0' '18_1_1' '19_0_0' '19_0_1' '19_1_0' '19_1_1' '20_0_0'\n '20_0_1' '20_1_0' '20_1_1' '24_0_0' '24_0_1' '24_1_0' '24_1_1' '25_0_0'\n '25_0_1' '25_1_0' '25_1_1' '26_0_0' '26_0_1' '26_1_0' '26_1_1' '27_0_0'\n '27_0_1' '27_1_0' '27_1_1' '28_0_0' '28_0_1' '28_1_0' '28_1_1' '30_0_0'\n '30_0_1' '30_1_0' '30_1_1' '31_0_0' '31_0_1' '31_1_0' '31_1_1' '35_0_0'\n '35_0_1' '35_1_0' '35_1_1' '38_0_0' '38_0_1' '38_1_0' '38_1_1' '39_0_0'\n '39_0_1' '39_1_0' '39_1_1' '41_0_0' '41_0_1' '41_1_0' '41_1_1' '42_0_0'\n '42_0_1' '42_1_0' '42_1_1' '43_0_0' '43_0_1' '43_1_0' '43_1_1' '44_0_0'\n '44_0_1' '44_1_0' '44_1_1' '47_0_0' '47_0_1' '47_1_0' '47_1_1' '48_0_0'\n '48_0_1' '48_1_0' '48_1_1' '50_0_0' '50_0_1' '50_1_0' '50_1_1' '51_0_0'\n '51_0_1' '51_1_0' '51_1_1' '60_0_0' '60_0_1' '60_1_0' '60_1_1' '61_0_0'\n '61_0_1' '61_1_0' '61_1_1' '62_0_0' '62_0_1' '62_1_0' '62_1_1' '64_0_0'\n '64_0_1' '64_1_0' '64_1_1' '65_0_0' '65_0_1' '65_1_0' '65_1_1' '97_0_0'\n '97_0_1' '97_1_0' '97_1_1' '106_0.0_0' '106_0.0_1' '106_0.05_0'\n '106_0.05_1' '106_0.1_0' '106_0.1_1' '106_0.15_0' '106_0.15_1'\n '106_0.2_0' '106_0.2_1' '106_0.25_0' '106_0.25_1' '106_0.3_0' '106_0.3_1'\n '106_0.35_0' '106_0.35_1' '106_0.4_0' '106_0.4_1' '106_0.45_0'\n '106_0.45_1' '106_0.5_0' '106_0.5_1' '106_0.55_0' '106_0.55_1'\n '106_0.6_0' '106_0.6_1' '106_0.65_0' '106_0.65_1' '106_0.7_0' '106_0.7_1'\n '106_0.75_0' '106_0.75_1' '106_0.8_0' '106_0.8_1' '106_0.85_0'\n '106_0.85_1' '106_0.9_0' '106_0.9_1' '106_0.95_0' '106_0.95_1'\n '106_1.0_0' '106_1.0_1' '107_0.0_0' '107_0.0_1' '107_0.043478261_0'\n '107_0.043478261_1' '107_0.086956522_0' '107_0.086956522_1'\n '107_0.130434783_0' '107_0.130434783_1' '107_0.173913043_0'\n '107_0.173913043_1' '107_0.217391304_0' '107_0.217391304_1'\n '107_0.260869565_0' '107_0.260869565_1' '107_0.304347826_0'\n '107_0.304347826_1' '107_0.347826087_0' '107_0.347826087_1'\n '107_0.391304348_0' '107_0.391304348_1' '107_0.434782609_0'\n '107_0.434782609_1' '107_0.47826087_0' '107_0.47826087_1'\n '107_0.52173913_0' '107_0.52173913_1' '107_0.565217391_0'\n '107_0.565217391_1' '107_0.608695652_0' '107_0.608695652_1'\n '107_0.652173913_0' '107_0.652173913_1' '107_0.695652174_0'\n '107_0.695652174_1' '107_0.739130435_0' '107_0.739130435_1'\n '107_0.782608696_0' '107_0.782608696_1' '107_0.826086957_0'\n '107_0.826086957_1' '107_0.869565217_0' '107_0.869565217_1'\n '107_0.956521739_0' '107_0.956521739_1' '107_1.0_0' '107_1.0_1'\n '117_0.0_0' '117_0.0_1' '117_0.25_0' '117_0.25_1' '117_0.5_0' '117_0.5_1'\n '117_0.75_0' '117_0.75_1' '117_1.0_0' '117_1.0_1' '120_0.0_0' '120_0.0_1'\n '120_0.010752688_0' '120_0.010752688_1' '120_0.021505376_0'\n '120_0.021505376_1' '120_0.032258065_0' '120_0.032258065_1'\n '120_0.043010753_0' '120_0.043010753_1' '120_0.053763441_0'\n '120_0.053763441_1' '120_0.064516129_0' '120_0.064516129_1'\n '120_0.075268817_0' '120_0.075268817_1' '120_0.086021505_0'\n '120_0.086021505_1' '120_0.096774194_0' '120_0.096774194_1'\n '120_0.107526882_0' '120_0.107526882_1' '124_0.0_0' '124_0.0_1'\n '124_0.25_0' '124_0.25_1' '124_0.5_0' '124_0.5_1' '124_0.75_0'\n '124_0.75_1' '124_1.0_0' '124_1.0_1' '127_0.0_0' '127_0.0_1'\n '127_0.010752688_0' '127_0.010752688_1' '127_0.021505376_0'\n '127_0.021505376_1' '127_0.032258065_0' '127_0.032258065_1'\n '127_0.043010753_0' '127_0.043010753_1' '127_0.053763441_0'\n '127_0.053763441_1' '127_0.064516129_0' '127_0.064516129_1'\n '127_0.075268817_0' '127_0.075268817_1' '127_0.086021505_0'\n '127_0.086021505_1' '127_0.096774194_0' '127_0.096774194_1' '128_0_0'\n '128_0_1' '128_1_0' '128_1_1' '130_0_0' '130_0_1' '130_1_0' '130_1_1'\n '131_0_0' '131_0_1' '131_1_0' '131_1_1' '132_0_0' '132_0_1' '132_1_0'\n '132_1_1' '137_0_0' '137_0_1' '137_1_0' '137_1_1' '138_0_0' '138_0_1'\n '138_1_0' '138_1_1' '139_0_0' '139_0_1' '139_1_0' '139_1_1' '140_0_1'\n '141_0_0' '141_0_1' '141_1_0' '141_1_1' '144_0_0' '144_0_1' '144_1_0'\n '144_1_1' '145_0_0' '145_0_1' '145_1_0' '145_1_1' '146_0_0' '146_0_1'\n '146_1_0' '146_1_1' '148_0_0' '148_0_1' '148_1_0' '148_1_1' '150_0_0'\n '150_0_1' '150_1_0' '150_1_1' '151_0_0' '151_0_1' '151_1_0' '151_1_1'\n '152_0_1' '156_0_0' '156_0_1' '156_1_0' '156_1_1' '161_0_0' '161_0_1'\n '161_1_0' '161_1_1' '162_0_0' '162_0_1' '162_1_0' '162_1_1' '163_0_0'\n '163_0_1' '163_1_0' '163_1_1' '164_0_1' '165_0_0' '165_0_1' '165_1_0'\n '165_1_1' '167_0_0' '167_0_1' '167_1_0' '167_1_1' '169_0_0' '169_0_1'\n '169_1_0' '169_1_1' '171_0_0' '171_0_1' '171_1_0' '171_1_1' '172_0_0'\n '172_0_1' '172_1_0' '172_1_1' '173_0_0' '173_0_1' '173_1_0' '173_1_1'\n '174_0_0' '174_0_1' '174_1_0' '174_1_1' '175_0_0' '175_0_1' '175_1_0'\n '175_1_1' '178_0_0' '178_0_1' '178_1_0' '178_1_1' '180_0_0' '180_0_1'\n '180_1_0' '180_1_1' '181_0_0' '181_0_1' '181_1_0' '181_1_1' '182_0_0'\n '182_0_1' '182_1_0' '182_1_1' '183_0_0' '183_0_1' '183_1_0' '183_1_1'\n '185_0_0' '185_0_1' '185_1_0' '185_1_1' '187_0_0' '187_0_1' '187_1_0'\n '187_1_1' '188_0_0' '188_0_1' '188_1_0' '188_1_1' '191_0_0' '191_0_1'\n '191_1_0' '191_1_1' '192_0_0' '192_0_1' '192_1_0' '192_1_1' '193_0_0'\n '193_0_1' '193_1_0' '193_1_1' '212_0.0_0' '212_0.0_1' '212_0.003846547_0'\n '212_0.003846547_1' '212_0.0260542_0' '212_0.0260542_1'\n '212_0.233191725_0' '212_0.233191725_1' '212_1.0_0' '212_1.0_1' '215_0_0'\n '215_0_1' '215_1_0' '215_1_1' '216_0_0' '216_0_1' '216_1_0' '216_1_1'\n '217_0_0' '217_0_1' '217_1_0' '217_1_1' '218_0_0' '218_0_1' '218_1_0'\n '218_1_1' '219_0_0' '219_0_1' '219_1_0' '219_1_1' '220_0_0' '220_0_1'\n '220_1_0' '220_1_1' '221_0_0' '221_0_1' '221_1_0' '221_1_1' '223_0_0'\n '223_0_1' '223_1_0' '223_1_1' '224_0_0' '224_0_1' '224_1_0' '224_1_1'\n '227_0_0' '227_0_1' '227_1_0' '227_1_1' '228_0_0' '228_0_1' '228_1_0'\n '228_1_1' '232_0_0' '232_0_1' '232_1_0' '232_1_1' '233_0_0' '233_0_1'\n '233_1_0' '233_1_1' '234_0_0' '234_0_1' '234_1_0' '234_1_1' '235_0_0'\n '235_0_1' '235_1_0' '235_1_1' '236_0_0' '236_0_1' '236_1_0' '236_1_1'\n '238_0_0' '238_0_1' '238_1_0' '238_1_1' '240_0_0' '240_0_1' '240_1_0'\n '240_1_1' '242_0_0' '242_0_1' '242_1_0' '242_1_1' '244_0_0' '244_0_1'\n '244_1_0' '244_1_1' '246_0_0' '246_0_1' '246_1_0' '246_1_1' '249_0_0'\n '249_0_1' '249_1_0' '249_1_1' '250_0_0' '250_0_1' '250_1_0' '250_1_1'\n '251_0_0' '251_0_1' '251_1_0' '251_1_1' '253_0_0' '253_0_1' '253_1_0'\n '253_1_1' '254_0_0' '254_0_1' '254_1_0' '254_1_1' '255_0_0' '255_0_1'\n '255_1_0' '255_1_1' '256_0_0' '256_0_1' '256_1_0' '256_1_1' '257_0_0'\n '257_0_1' '257_1_0' '257_1_1' '258_0_0' '258_0_1' '258_1_0' '258_1_1'\n '259_0_0' '259_0_1' '259_1_0' '259_1_1' '260_0_0' '260_0_1' '260_1_0'\n '260_1_1' '263_0_0' '263_0_1' '263_1_0' '263_1_1' '265_0_0' '265_0_1'\n '265_1_0' '265_1_1' '268_0_0' '268_0_1' '268_1_0' '268_1_1' '269_0_0'\n '269_0_1' '269_1_0' '269_1_1' '270_0_0' '270_0_1' '270_1_0' '270_1_1'\n '273_0_0' '273_0_1' '273_1_0' '273_1_1' '274_0_0' '274_0_1' '274_1_0'\n '274_1_1' '275_0_0' '275_0_1' '275_1_0' '275_1_1' '276_0_0' '276_0_1'\n '276_1_0' '276_1_1' '277_0_0' '277_0_1' '277_1_0' '277_1_1' '278_0_0'\n '278_0_1' '278_1_0' '278_1_1' '284_0_0' '284_0_1' '284_1_0' '284_1_1'\n '285_0_0' '285_0_1' '285_1_0' '285_1_1' '287_0_0' '287_0_1' '287_1_0'\n '287_1_1' '289_0_0' '289_0_1' '289_1_0' '289_1_1' '290_0_0' '290_0_1'\n '290_1_0' '290_1_1' '294_0_0' '294_0_1' '294_1_0' '294_1_1' '296_0_0'\n '296_0_1' '296_1_0' '296_1_1' '297_0_0' '297_0_1' '297_1_0' '297_1_1'\n '298_0_0' '298_0_1' '298_1_0' '298_1_1' '300_0_0' '300_0_1' '300_1_0'\n '300_1_1' '301_0_0' '301_0_1' '301_1_0' '301_1_1' '302_0_0' '302_0_1'\n '302_1_0' '302_1_1' '303_0_0' '303_0_1' '303_1_0' '303_1_1' '304_0_0'\n '304_0_1' '304_1_0' '304_1_1' '305_0_0' '305_0_1' '305_1_0' '305_1_1'\n '308_0_0' '308_0_1' '308_1_0' '308_1_1' '309_0_0' '309_0_1' '309_1_0'\n '309_1_1' '312_0.0_0' '312_0.0_1' '312_0.39424_0' '312_0.39424_1'\n '312_0.4324508_0' '312_0.4324508_1' '312_0.47097_0' '312_0.47097_1'\n '312_0.5197096_0' '312_0.5197096_1' '312_0.5722851_0' '312_0.5722851_1'\n '312_0.588824_0' '312_0.588824_1' '312_0.6608119_0' '312_0.6608119_1'\n '312_0.7575137_0' '312_0.7575137_1' '312_0.8421118_0' '312_0.8421118_1'\n '312_0.9310675_0' '312_0.9310675_1' '312_1.0_0' '312_1.0_1' '316_0_0'\n '316_0_1' '316_1_0' '316_1_1' '332_0_0' '332_0_1' '332_1_0' '332_1_1'\n '338_0_0' '338_0_1' '338_1_0' '338_1_1' '339_0_0' '339_0_1' '339_1_0'\n '339_1_1' '341_0.0_0' '341_0.0_1' '341_0.111111111_0' '341_0.111111111_1'\n '341_0.222222222_0' '341_0.222222222_1' '341_0.333333333_0'\n '341_0.333333333_1' '341_0.444444444_0' '341_0.444444444_1'\n '341_0.555555556_0' '341_0.555555556_1' '341_0.666666667_0'\n '341_0.666666667_1' '341_0.777777778_0' '341_0.777777778_1'\n '341_0.888888889_0' '341_0.888888889_1' '341_1.0_0' '341_1.0_1' '342_0_0'\n '342_0_1' '342_1_0' '342_1_1' '344_0_0' '344_0_1' '344_1_0' '344_1_1'] not contained in axis"
     ]
    }
   ],
   "source": [
    "X_tt = pd.get_dummies(data=X_val, columns=cat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_to_file(clf.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(model):\n",
    "    \n",
    "    X_train_f = X_train\n",
    "    X_val_f = X_val\n",
    "    #X_train_f = X_train.iloc[:, :10]\n",
    "    #X_val_f = X_val.iloc[:, :10]\n",
    "    count = 1\n",
    "    drop_features = []\n",
    "    max_score = roc_auc_345\n",
    "    #max_score = 0\n",
    "    \n",
    "    #while True\n",
    "    for f in range(344):\n",
    "    \n",
    "        feature_score = {}\n",
    "\n",
    "        # выкидываем 1 признак, считаем скор\n",
    "        for i in tqdm(X_train_f.columns):\n",
    "            X_train1 = X_train_f.drop(i, axis=1)\n",
    "            X_val1 = X_val_f.drop(i, axis=1)\n",
    "            model.fit(X_train1, y_train)\n",
    "            feature_score[i] = roc_auc_score(y_val, logit.predict_proba(X_val1)[:,1])\n",
    "\n",
    "        # находим признак, выкидывание которого дает максимальный скор\n",
    "        feature = max(feature_score, key=feature_score.get)\n",
    "        score = feature_score[feature]\n",
    "        \n",
    "        # если максимальный скор меньше, то на выход\n",
    "        if score <= max_score:\n",
    "            print('Complete!')\n",
    "            break\n",
    "            \n",
    "        drop_features.append(feature)\n",
    "        max_score = score\n",
    "        \n",
    "        # убираем этот признак\n",
    "        X_train_f = X_train_f.drop(feature, axis=1)\n",
    "        X_val_f = X_val_f.drop(feature, axis=1)\n",
    "\n",
    "        print('Stage {}, drop_feature {}, roc_auc_score {}'.format(count, feature, score))\n",
    "\n",
    "        count += 1\n",
    "        \n",
    "    return drop_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "clf = MLPRegressor(solver='lbfgs', alpha=1e-2,hidden_layer_sizes=(4, 1), random_state=1,activation='logistic',learning_rate='invscaling',learning_rate_init=0.00001,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7443866657980899"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X, Y)  \n",
    "roc_auc_score(Y, clf.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_to_file(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "class DjStacking(BaseEstimator, ClassifierMixin):  \n",
    "    \"\"\"Стэкинг моделей scikit-learn\"\"\"\n",
    "\n",
    "    def __init__(self, models, ens_model):\n",
    "        \"\"\"\n",
    "        Инициализация\n",
    "        models - базовые модели для стекинга\n",
    "        ens_model - мета-модель\n",
    "        \"\"\"\n",
    "        self.models = models\n",
    "        self.ens_model = ens_model\n",
    "        self.n = len(models)\n",
    "        self.valid = None\n",
    "        \n",
    "    def fit(self, X, y=None, p=0.25, cv=3, err=0.001, random_state=None):\n",
    "        \"\"\"\n",
    "        Обучение стекинга\n",
    "        p - в каком отношении делить на обучение / тест\n",
    "            если p = 0 - используем всё обучение!\n",
    "        cv  (при p=0) - сколько фолдов использовать\n",
    "        err (при p=0) - величина случайной добавки к метапризнакам\n",
    "        random_state - инициализация генератора\n",
    "            \n",
    "        \"\"\"\n",
    "        if (p > 0): # делим на обучение и тест\n",
    "            # разбиение на обучение моделей и метамодели\n",
    "            train, valid, y_train, y_valid = train_test_split(X, y, test_size=p, random_state=random_state)\n",
    "            \n",
    "            # заполнение матрицы для обучения метамодели\n",
    "            self.valid = np.zeros((valid.shape[0], self.n))\n",
    "            for t, clf in enumerate(self.models):\n",
    "                clf.fit(train, y_train)\n",
    "                self.valid[:, t] = clf.predict(valid)\n",
    "                \n",
    "            # обучение метамодели\n",
    "            self.ens_model.fit(self.valid, y_valid)\n",
    "            \n",
    "        else: # используем всё обучение\n",
    "            \n",
    "            # для регуляризации - берём случайные добавки\n",
    "            self.valid = err*np.random.randn(X.shape[0], self.n)\n",
    "            \n",
    "            for t, clf in enumerate(self.models):\n",
    "                # это oob-ответы алгоритмов\n",
    "                self.valid[:, t] += cross_val_predict(clf, X, y, cv=cv, n_jobs=-1, method='predict')\n",
    "                # но сам алгоритм надо настроить\n",
    "                clf.fit(X, y)\n",
    "            \n",
    "            # обучение метамодели\n",
    "            self.ens_model.fit(self.valid, y)  \n",
    "            \n",
    "\n",
    "        return self\n",
    "    \n",
    "\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Работа стэкинга\n",
    "        \"\"\"\n",
    "        # заполение матрицы для мета-классификатора\n",
    "        X_meta = np.zeros((X.shape[0], self.n))\n",
    "        \n",
    "        for t, clf in enumerate(self.models):\n",
    "            X_meta[:, t] = clf.predict(X)\n",
    "        \n",
    "        a = self.ens_model.predict(X_meta)\n",
    "        \n",
    "        return (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def run_and_plot(clf, X, y, label):\n",
    "    # plt.figure(figsize=(6, 5))\n",
    "    \n",
    "    a = clf.predict(X)\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.linspace(-3, 3, 500), np.linspace(-3, 3, 500))\n",
    "\n",
    "    \n",
    "    print (label + ' AUC-ROC  = ' + str( roc_auc_score(y, a) ))\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(8, 7))\n",
    "    plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), Z.max(), 20), cmap=plt.cm.bwr, alpha=0.3) # plt.cm.Blues_r cmap=plt.cm.Blues_r)\n",
    "    #a_ = plt.contour(xx, yy, Z, levels=[threshold], linewidths=1, colors='black')\n",
    "    #plt.contourf(xx, yy, Z, levels=[threshold, Z.max()], colors='#CCDDFF')\n",
    "    \n",
    "    # не всё... :100\n",
    "    plt.scatter(X[:300, 0], X[:300, 1], c=y[:300], s=20, alpha=1.0)\n",
    "    plt.xlim([-3, 3])\n",
    "    plt.ylim([-3, 3])\n",
    "    # plt.axis('tight')\n",
    "    plt.axis('off')\n",
    "    plt.title(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector=SelectKBest(chi2, k=76)\n",
    "selector.fit(X,Y)\n",
    "X_new=selector.transform(X)\n",
    "X_v=selector.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.541551282801808"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold \n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, RidgeCV, LassoCV, Ridge, Lasso\n",
    "seed=42\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "logit_grid = LogisticRegressionCV(Cs=[2], cv=cv, scoring = 'roc_auc', verbose=True, n_jobs=-1)\n",
    "logit_grid.fit(train_X, train_y)\n",
    "roc_auc_score(test_y,logit_grid.predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X, Y, train_size=0.7, random_state=1999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "       learning_rate=0.05, max_depth=5, min_child_samples=20,\n",
       "       min_child_weight=0.001, min_split_gain=0.0, n_estimators=200,\n",
       "       n_jobs=-1, nthread=-1, num_leaves=31, objective='regression',\n",
       "       random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "       subsample=1.0, subsample_for_bin=200000, subsample_freq=1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "\n",
    "knn1= MLPRegressor(solver='lbfgs', alpha=1e-2,hidden_layer_sizes=(4, 1), random_state=1,activation='logistic',learning_rate='invscaling',learning_rate_init=0.00001,verbose=1)\n",
    "knn1.fit(train_X, train_y)\n",
    "#run_and_plot(knn1, test_X, test_y, '3NN')\n",
    "\n",
    "knn2 = KNeighborsRegressor(n_neighbors=10)\n",
    "knn2.fit(train_X, train_y)\n",
    "#run_and_plot(knn2, test_X, test_y, '10NN')\n",
    "\n",
    "\n",
    "rg0 = Ridge(alpha=0.01)\n",
    "rg0.fit(train_X, train_y)\n",
    "#run_and_plot(rg0, test_X, test_y, 'ridge-0.01')\n",
    "\n",
    "rg1 = Ridge(alpha=1.1)\n",
    "rg1.fit(train_X, train_y)\n",
    "#run_and_plot(rg1, test_X, test_y, 'ridge-1.1')\n",
    "\n",
    "rg2 = Ridge(alpha=100.1)\n",
    "rg2.fit(train_X, train_y)\n",
    "#run_and_plot(rg2, test_X, test_y, 'ridge-100.1')\n",
    "\n",
    "\n",
    "rf1 = RandomForestRegressor(n_estimators=1000, max_depth=1)\n",
    "rf1.fit(train_X, train_y)\n",
    "#run_and_plot(rf1, test_X, test_y, 'rf-d1')\n",
    "\n",
    "rf2 = RandomForestRegressor(n_estimators=100, max_depth=5)\n",
    "rf2.fit(train_X, train_y)\n",
    "#run_and_plot(rf2, test_X, test_y, 'rf-d5')\n",
    "\n",
    "\n",
    "gbm1 = lgb.LGBMRegressor(boosting_type='gbdt', learning_rate=0.05, max_depth=2, n_estimators=200, nthread=-1, objective='regression')    \n",
    "gbm1.fit(train_X, train_y)\n",
    "#run_and_plot(gbm1, test_X, test_y, 'gbm-d2')\n",
    "\n",
    "gbm2 = lgb.LGBMRegressor(boosting_type='gbdt', learning_rate=0.05, max_depth=5, n_estimators=200, nthread=-1, objective='regression')    \n",
    "gbm2.fit(train_X, train_y)\n",
    "#run_and_plot(gbm2, test_X, test_y, 'gbm-d5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DjStacking(ens_model=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001),\n",
       "      models=[MLPRegressor(activation='logistic', alpha=0.01, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(4, 1), learning_rate='invscaling',\n",
       "       learning_rate_init=1e-05, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, ...0.0, reg_lambda=0.0, silent=True,\n",
       "       subsample=1.0, subsample_for_bin=200000, subsample_freq=1)])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [knn1, knn2,rg1, rg2, rf1, rf2, gbm1, gbm2] # , rf3\n",
    "ens_model = Ridge()\n",
    "s1 = DjStacking(models, ens_model)\n",
    "s1.fit(train_X, train_y)\n",
    "#run_and_plot(s1, test_X, test_y, '1-stacking')\n",
    "\n",
    "\n",
    "s2 = DjStacking(models, ens_model)\n",
    "s2.fit(train_X, train_y, p=-1)\n",
    "#run_and_plot(s1, test_X, test_y, '2-stacking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7350441474249071"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(test_y,s1.predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07189956, 0.33756426, 0.30236089, ..., 0.05579954, 0.08910986,\n",
       "       0.16609428])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.fit(X,Y)\n",
    "s1.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_to_file(pred):\n",
    "    pred = pd.DataFrame(pred, columns=[\"_VAL_\"])\n",
    "    pred.to_csv('solution_stack.csv', index_label=\"_ID_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_to_file(s1.predict(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7330116788108415"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(test_y,s2.predict(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.743"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.727856157704744"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn1= MLPRegressor(solver='lbfgs', alpha=1e-2,hidden_layer_sizes=(4, 1), random_state=1,activation='logistic',learning_rate='invscaling',learning_rate_init=0.00001,verbose=1)\n",
    "knn1.fit(train_X, train_y)\n",
    "#run_and_plot(knn1, test_X, test_y, '3NN')\n",
    "\n",
    "knn2 = KNeighborsRegressor(n_neighbors=10)\n",
    "knn2.fit(train_X, train_y)\n",
    "#run_and_plot(knn2, test_X, test_y, '10NN')\n",
    "\n",
    "\n",
    "rg0 = Ridge(alpha=0.01)\n",
    "rg0.fit(train_X, train_y)\n",
    "#run_and_plot(rg0, test_X, test_y, 'ridge-0.01')\n",
    "\n",
    "rg1 = Ridge(alpha=1.1)\n",
    "rg1.fit(train_X, train_y)\n",
    "#run_and_plot(rg1, test_X, test_y, 'ridge-1.1')\n",
    "\n",
    "rg2 = Ridge(alpha=100.1)\n",
    "rg2.fit(train_X, train_y)\n",
    "#run_and_plot(rg2, test_X, test_y, 'ridge-100.1')\n",
    "\n",
    "bayes=GaussianNB()\n",
    "bayes.fit(train_X,train_y)\n",
    "\n",
    "rf1 = RandomForestRegressor(n_estimators=100, max_depth=10)\n",
    "rf1.fit(train_X, train_y)\n",
    "#run_and_plot(rf1, test_X, test_y, 'rf-d1')\n",
    "\n",
    "rf2 = RandomForestRegressor(n_estimators=100, max_depth=5)\n",
    "rf2.fit(train_X, train_y)\n",
    "#run_and_plot(rf2, test_X, test_y, 'rf-d5')\n",
    "\n",
    "\n",
    "gbm1 = lgb.LGBMRegressor(boosting_type='gbdt', learning_rate=0.005, max_depth=2, n_estimators=200, nthread=-1, objective='regression')    \n",
    "gbm1.fit(train_X, train_y)\n",
    "#run_and_plot(gbm1, test_X, test_y, 'gbm-d2')\n",
    "\n",
    "gbm2 = lgb.LGBMRegressor(boosting_type='gbdt', learning_rate=0.005, max_depth=5, n_estimators=200, nthread=-1, objective='regression')    \n",
    "gbm2.fit(train_X, train_y)\n",
    "#run_and_plot(gbm2, test_X, test_y, 'gbm-d5')\n",
    "\n",
    "models = [knn1, knn2,rg1, rg2,bayes, rf1, rf2, gbm1, gbm2] # , rf3\n",
    "ens_model = lgb.LGBMRegressor(boosting_type='gbdt', learning_rate=0.005, max_depth=2, n_estimators=200, nthread=-1, objective='regression')    \n",
    "s1 = DjStacking(models, ens_model)\n",
    "s1.fit(train_X, train_y)\n",
    "#run_and_plot(s1, test_X, test_y, '1-stacking')\n",
    "\n",
    "\n",
    "s2 = DjStacking(models, ens_model)\n",
    "s2.fit(train_X, train_y, p=-1)\n",
    "#run_and_plot(s1, test_X, test_y, '2-stacking')\n",
    "\n",
    "roc_auc_score(test_y,s1.predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
